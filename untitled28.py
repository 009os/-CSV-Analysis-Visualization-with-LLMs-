# -*- coding: utf-8 -*-
"""Untitled28.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sCclVzLqUJ9JQcn3QKbzRIGpEIWSWYsZ
"""

import pandas as pd
import matplotlib.pyplot as plt
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Function to read and parse CSV files
def read_csv(file_path):
    return pd.read_csv("/content/data_by_artist.csv")

# Function to calculate basic statistics
def calculate_statistics(data):
    # Select only numeric columns for calculations
    numeric_data = data.select_dtypes(include=['number'])

    statistics = {
        'mean': numeric_data.mean(),
        'median': numeric_data.median(),
        'mode': numeric_data.mode().iloc[0],
        'std': numeric_data.std(),
        'correlation': numeric_data.corr()
    }
    return statistics

# Function to generate plots
def plot_data(data):
    # Histograms for numeric columns
    numeric_data = data.select_dtypes(include=['number'])
    numeric_data.hist(bins=15, figsize=(15, 10))
    plt.show()

    # Scatter Matrix for numeric columns
    pd.plotting.scatter_matrix(numeric_data, figsize=(15, 10))
    plt.show()

# Function to answer questions using LLM
def answer_question(prompt, model, tokenizer):
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(inputs["input_ids"], max_length=100)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)



# Main function to integrate everything
def main():
    file_path = '/content/data_by_artist.csv'  # Replace with your actual file path
    data = read_csv(file_path)

    print("Data Head:")
    print(data.head())

    stats = calculate_statistics(data)
    print("Statistics:")
    print(stats)

    plot_data(data)

    model_name = "google/flan-t5-small"  # Replace with a publicly available model
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    # Use AutoModelForSeq2SeqLM for sequence-to-sequence models like T5
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

    question = "What is the median of the data?"
    response = answer_question(f"Question: {question}\nAnswer:", model, tokenizer)
    print("Response from LLM:")
    print(response)

if __name__ == "__main__":
    main()









